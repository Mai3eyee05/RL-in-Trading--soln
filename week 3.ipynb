{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 1 \n",
    "## Modeling simple RL problems by making their MDPs in Python\n",
    "\n",
    "We will create the MDPs for some of the example problems from Grokking textbook. For the simple environments, we can just hardcode the MDPs into a dictionary by exhaustively encoding the whole state space and the transition function. We will also go through a more complicated example where the state space is too large to be manually coded and we need to implement the transition function based on some state parameters.\n",
    "\n",
    "Later on, you will not need to implement the MDPs of common RL problems yourself, most of the work is already done by the OpenAI Gym library, which includes models for most of the famous RL envis.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 0 - Bandit Walk\n",
    "\n",
    "Let us consider the BW environment on Page 39. \n",
    "\n",
    "State Space has 3 elements, states 0, 1 and 2.\n",
    "States 0 and 2 are terminal states and state 1 is the starting state.\n",
    "\n",
    "Action space has 2 elements, left and right.\n",
    "\n",
    "The environment is deterministic - transition probability of any action is 1.\n",
    "\n",
    "Only 1 (State, Action, State') tuple has positive reward, (1, Right, 2) gives the agent +1 reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll model this MDP as a dictionary. This code is an example for the upcoming exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_mdp = {\n",
    "\n",
    "    0 : {\n",
    "        \"Right\" : [(1, 0, 0, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "\n",
    "    1 : {\n",
    "        \"Right\" : [(1, 2, 1, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "\n",
    "    2 : {\n",
    "        \"Right\" : [(1, 2, 0, True)],\n",
    "        \"Left\" : [(1, 2, 0, True)]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by convention, all actions from terminal states still lead to the same state with reward 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 1 - Slippery Walk\n",
    "\n",
    "Let us now look at the BSW environment on Page 40. We'll model a slightly modified version of BSW with 7 states instead (i.e the SWF envi on Page 67). It will be useful in the coming weeks.\n",
    "\n",
    "Here, states 0 and 6 are terminal states and state 3 is the starting state.\n",
    "\n",
    "Action space has again 2 elements, left and right.\n",
    "\n",
    "The environment is now stochastic, transition probability of any action is as follows -\n",
    "If agent chooses `Right` at a non-terminal state,\n",
    "- $50\\%$ times it will go to `Right` state\n",
    "- $33\\frac{1}{3} \\%$ times it will stay in same state\n",
    "- $16\\frac{2}{3}\\%$ times it will go to `Left`state\n",
    "\n",
    "This time, 2 different (State, Action, State') tuples have positive rewards, you need to find them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll again model this MDP as a dictionary. Part of the code is written for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "swf_mdp = {}\n",
    "\n",
    "\n",
    "swf_mdp[0] = {\n",
    "    \"Right\": [(1.0, 0, 0, True)],\n",
    "    \"Left\":  [(1.0, 0, 0, True)]\n",
    "}\n",
    "\n",
    "swf_mdp[6] = {\n",
    "    \"Right\": [(1.0, 6, 0, True)],\n",
    "    \"Left\":  [(1.0, 6, 0, True)]\n",
    "}\n",
    "\n",
    "swf_mdp[1] = {\n",
    "    \"Right\": [\n",
    "        (1/2, 2, 0, False), \n",
    "        (1/3, 1, 0, False), \n",
    "        (1/6, 0, 0, True)   \n",
    "    ],\n",
    "    \"Left\": [\n",
    "        (1/2, 0, 0, True),  \n",
    "        (1/3, 1, 0, False), \n",
    "        (1/6, 2, 0, False)  \n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "for s in range(2, 6):\n",
    "    is_left_terminal = (s - 1 == 0)\n",
    "    is_right_terminal = (s + 1 == 6)\n",
    "    \n",
    "   \n",
    "    reward_val = 1.0 if (s + 1 == 6) else 0.0\n",
    "    \n",
    "    swf_mdp[s] = {\n",
    "        \"Right\": [\n",
    "            (1/2, s + 1, reward_val, is_right_terminal),\n",
    "            (1/3, s,     0,          False),            \n",
    "            (1/6, s - 1, 0,          is_left_terminal)  \n",
    "        ],\n",
    "        \"Left\": [\n",
    "            (1/2, s - 1, 0,          is_left_terminal), \n",
    "            (1/3, s,     0,          False),            \n",
    "            (1/6, s + 1, reward_val, is_right_terminal) \n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to automate filling this MDP, but ensure that it is correctly filled as it'll be back in next week's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 2 - Frozen Lake Environment\n",
    "\n",
    "This environment is described on Page 46.\n",
    "\n",
    "The FL environment has a large state space, so it's better to generate most of the MDP via Python instead of typing stuff manually.\n",
    "\n",
    "Note that all 5 states - 5, 7, 11, 12, 15 are terminal states, so keep that in mind while constructing the MDP.\n",
    "\n",
    "There are 4 actions now - Up, Down, Left, Right.\n",
    "\n",
    "The environment is stochastic, and states at the border of lake will require separate treatment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again we will model this MDP as a (large) dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_mdp = {}\n",
    "\n",
    "\n",
    "holes = [5, 7, 11, 12]\n",
    "goal = 15\n",
    "terminals = holes + [goal]\n",
    "\n",
    "\n",
    "actions = {\"Left\": (0, -1), \"Down\": (1, 0), \"Right\": (0, 1), \"Up\": (-1, 0)}\n",
    "\n",
    "\n",
    "slips = {\n",
    "    \"Left\":  [\"Up\", \"Down\"],\n",
    "    \"Down\":  [\"Left\", \"Right\"],\n",
    "    \"Right\": [\"Up\", \"Down\"],\n",
    "    \"Up\":    [\"Left\", \"Right\"]\n",
    "}\n",
    "\n",
    "for s in range(16):\n",
    "    fl_mdp[s] = {}\n",
    "    \n",
    "    \n",
    "    if s in terminals:\n",
    "        for a in actions:\n",
    "            fl_mdp[s][a] = [(1.0, s, 0, True)]\n",
    "        continue\n",
    "\n",
    "    \n",
    "    row, col = s // 4, s % 4\n",
    "    for a, (dr, dc) in actions.items():\n",
    "        outcomes = []\n",
    "       \n",
    "        possible_moves = [a] + slips[a]\n",
    "        \n",
    "        for move in possible_moves:\n",
    "            mdr, mdc = actions[move]\n",
    "            nr, nc = row + mdr, col + mdc\n",
    "            \n",
    "            \n",
    "            if 0 <= nr < 4 and 0 <= nc < 4:\n",
    "                next_s = nr * 4 + nc\n",
    "            else:\n",
    "                next_s = s\n",
    "            \n",
    "            is_done = next_s in terminals\n",
    "            reward = 1.0 if next_s == goal else 0.0\n",
    "            outcomes.append((1/3, next_s, reward, is_done))\n",
    "            \n",
    "        fl_mdp[s][a] = outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in range(0, 16):\n",
    "    \n",
    "    transitions = {}\n",
    "    \n",
    "   \n",
    "    holes = [5, 7, 11, 12]\n",
    "    goal = 15\n",
    "    \n",
    "    \n",
    "    if state in holes or state == goal:\n",
    "        for action in [\"Up\", \"Down\", \"Right\", \"Left\"]:\n",
    "            transitions[action] = [(1.0, state, 0, True)]\n",
    "        fl_mdp[state] = transitions\n",
    "        continue\n",
    "\n",
    "   \n",
    "    row, col = state // 4, state % 4\n",
    "\n",
    "    for action in [\"Up\", \"Down\", \"Right\", \"Left\"]:\n",
    "        \n",
    "        if action == \"Up\" or action == \"Down\":\n",
    "            possible_actions = [action, \"Left\", \"Right\"]\n",
    "        else:\n",
    "            possible_actions = [action, \"Up\", \"Down\"]\n",
    "            \n",
    "        outcomes = []\n",
    "        for act in possible_actions:\n",
    "            \n",
    "            new_row, new_col = row, col\n",
    "            if act == \"Up\":    new_row = max(0, row - 1)\n",
    "            if act == \"Down\":  new_row = min(3, row + 1)\n",
    "            if act == \"Left\":  new_col = max(0, col - 1)\n",
    "            if act == \"Right\": new_col = min(3, col + 1)\n",
    "            \n",
    "            new_state = new_row * 4 + new_col\n",
    "            \n",
    "            \n",
    "            done = (new_state in holes or new_state == goal)\n",
    "            reward = 1.0 if new_state == goal else 0.0\n",
    "            \n",
    "            outcomes.append((1/3, new_state, reward, done))\n",
    "            \n",
    "        transitions[action] = outcomes\n",
    "\n",
    "    fl_mdp[state] = transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to do some stuff manually, but make sure to automate most of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation of the FL environment by comparing it with the one in OpenAI Gym.\n",
    "\n",
    "You don't need to worry about Gym right now, we'll set it up in the coming weeks. But here is the code to import an MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium\n",
    "#import gymnasium as gym\n",
    "#P = gym.make('FrozenLake-v1').env.P\n",
    "\n",
    "import gymnasium as gym\n",
    "import pprint\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v1')\n",
    "P = env.unwrapped.P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the imported MDP is also just a dictionary, we can just print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed the MDP! Transitions for State 14:\n",
      "{0: [(0.33333333333333337, 10, 0, False),\n",
      "     (0.3333333333333333, 13, 0, False),\n",
      "     (0.33333333333333337, 14, 0, False)],\n",
      " 1: [(0.33333333333333337, 13, 0, False),\n",
      "     (0.3333333333333333, 14, 0, False),\n",
      "     (0.33333333333333337, 15, 1, True)],\n",
      " 2: [(0.33333333333333337, 14, 0, False),\n",
      "     (0.3333333333333333, 15, 1, True),\n",
      "     (0.33333333333333337, 10, 0, False)],\n",
      " 3: [(0.33333333333333337, 15, 1, True),\n",
      "     (0.3333333333333333, 10, 0, False),\n",
      "     (0.33333333333333337, 13, 0, False)]}\n"
     ]
    }
   ],
   "source": [
    "# using the pretty print module\n",
    "\n",
    "print(\"Success\")\n",
    "pprint.pprint(P[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
